# project2_motobike_app.py
# -*- coding: utf-8 -*-
"""
Project 2: ƒê·ªÅ xu·∫•t xe m√°y d·ª±a tr√™n n·ªôi dung, ph√¢n c·ª•m xe m√°y
Ch·∫°y tr√™n Visual Studio Code / Streamlit.
"""

import os
import re
import pickle
from math import ceil
from difflib import SequenceMatcher

import numpy as np
import pandas as pd
import streamlit as st

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

import matplotlib.pyplot as plt


# ======================================================
# C√ÅC H·∫∞NG S·ªê ƒê∆Ø·ªúNG D·∫™N
# ======================================================

CLUSTER_DATA_FILE = "data_motobikes_clean.xlsx"  # d·ªØ li·ªáu cho ph√¢n c·ª•m
SEARCH_DATA_FILE = "data_motobikes.xlsx"         # d·ªØ li·ªáu cho h·ªá g·ª£i √Ω
COSINE_PKL_FILE = "cosine_sim_model.pkl"         # ma tr·∫≠n cosine
HEADER_IMAGE_FILE = "xe.png"                     # ·∫£nh header chung (n·∫øu c√≥)
SEARCH_HEADER_IMAGE_FILE = "b12bca47-fea2-499d-80f1-1915896b8525.png"  # ·∫£nh trang t√¨m ki·∫øm (n·∫øu c√≥)


# ======================================================
# H√ÄM TI·ªÜN √çCH CHUNG
# ======================================================

def get_file_path(filename: str) -> str:
    """Tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi t·ªõi file n·∫±m c√πng th∆∞ m·ª•c v·ªõi script."""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    return os.path.join(script_dir, filename)


def get_image_path(filename: str):
    """Tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n ·∫£nh n·∫øu t·ªìn t·∫°i, ng∆∞·ª£c l·∫°i tr·∫£ v·ªÅ None."""
    path = get_file_path(filename)
    if os.path.exists(path):
        return path
    return None


# ======================================================
# PH·∫¶N 1: PH√ÇN C·ª§M XE M√ÅY (KMEANS + PCA + FORM D·ª∞ ƒêO√ÅN)
# ======================================================

def parse_price_to_million(s: str):
    """Chu·∫©n h√≥a chu·ªói gi√° v·ªÅ ƒë∆°n v·ªã tri·ªáu ƒë·ªìng."""
    if pd.isna(s):
        return np.nan
    s = str(s).lower()

    # lo·∫°i b·ªè k√Ω t·ª± kh√¥ng c·∫ßn thi·∫øt
    s = s.replace("\u00a0", " ")
    s = s.replace("vnƒë", "").replace("vnd", "").replace("ƒë", "")
    s = s.replace(",", ".").strip()

    m = re.search(r"(\d+\.?\d*)", s)
    if not m:
        return np.nan

    num = float(m.group(1))

    if "tri·ªáu" in s or " tr" in s:
        return num
    if "ngh√¨n" in s or "ng√†n" in s or "k" in s:
        return num / 1000
    # n·∫øu ch·ªâ ghi d·∫°ng 20.000.000
    if num > 1000:
        return num / 1_000_000
    return num


@st.cache_data
def load_and_prepare_cluster_data(data_path: str):
    """
    ƒê·ªçc & x·ª≠ l√Ω d·ªØ li·ªáu ph√¢n c·ª•m,
    tr·∫£ v·ªÅ df, numeric_cols, categorical_cols, preprocess, X_dense.
    """
    ext = os.path.splitext(data_path)[1].lower()
    if ext == ".csv":
        df_raw = pd.read_csv(data_path)
    else:
        df_raw = pd.read_excel(data_path)

    df = df_raw.copy()

    # T·ª± t√¨m c·ªôt kho·∫£ng gi√° min / max
    min_col_txt = [c for c in df.columns if "min" in c.lower()][0]
    max_col_txt = [c for c in df.columns if "max" in c.lower()][0]

    df["Kho·∫£ng gi√° min (tri·ªáu)"] = df[min_col_txt].apply(parse_price_to_million)
    df["Kho·∫£ng gi√° max (tri·ªáu)"] = df[max_col_txt].apply(parse_price_to_million)

    # C·ªôt Gi√° ch√≠nh
    if "Gi√°" in df.columns:
        df["Gi√°"] = pd.to_numeric(df["Gi√°"], errors="coerce")
        mask = df["Gi√°"].isna()
        df.loc[mask, "Gi√°"] = df.loc[
            mask, ["Kho·∫£ng gi√° min (tri·ªáu)", "Kho·∫£ng gi√° max (tri·ªáu)"]
        ].mean(axis=1)
    else:
        df["Gi√°"] = df[["Kho·∫£ng gi√° min (tri·ªáu)", "Kho·∫£ng gi√° max (tri·ªáu)"]].mean(axis=1)

    # Tu·ªïi xe
    df["NƒÉm ƒëƒÉng k√Ω"] = pd.to_numeric(df["NƒÉm ƒëƒÉng k√Ω"], errors="coerce")
    df["Tu·ªïi xe"] = 2025 - df["NƒÉm ƒëƒÉng k√Ω"]

    # Km
    if "S·ªë Km ƒë√£ ƒëi" in df.columns:
        df["S·ªë Km ƒë√£ ƒëi"] = pd.to_numeric(df["S·ªë Km ƒë√£ ƒëi"], errors="coerce")
    else:
        df["S·ªë Km ƒë√£ ƒëi"] = np.nan

    # C√°c c·ªôt d√πng ph√¢n c·ª•m
    numeric_cols = ["Gi√°", "Tu·ªïi xe", "S·ªë Km ƒë√£ ƒëi"]
    categorical_cols = [
        "Th∆∞∆°ng hi·ªáu",
        "D√≤ng xe",
        "Lo·∫°i xe",
        "Dung t√≠ch xe",
        "Xu·∫•t x·ª©",
    ]

    numeric_cols = [c for c in numeric_cols if c in df.columns]
    categorical_cols = [c for c in categorical_cols if c in df.columns]

    X = df[numeric_cols + categorical_cols].copy()

    pre_num = Pipeline(
        [
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler()),
        ]
    )
    pre_cat = Pipeline(
        [
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore")),
        ]
    )

    preprocess = ColumnTransformer(
        [
            ("num", pre_num, numeric_cols),
            ("cat", pre_cat, categorical_cols),
        ]
    )

    X_prep = preprocess.fit_transform(X)
    X_dense = X_prep.toarray()

    return df, numeric_cols, categorical_cols, preprocess, X_dense


def run_kmeans(df, numeric_cols, X_dense, K: int):
    """Ch·∫°y KMeans, tr·∫£ v·ªÅ model + k·∫øt qu·∫£."""
    model = KMeans(n_clusters=K, n_init=10, random_state=42)
    labels = model.fit_predict(X_dense)

    sil = silhouette_score(X_dense, labels)

    dfc = df.copy()
    dfc["cluster"] = labels

    summary = (
        dfc.groupby("cluster")[numeric_cols]
        .agg(["count", "mean", "min", "max"])
        .round(2)
    )

    pca = PCA(n_components=2, random_state=42)
    X_pca = pca.fit_transform(X_dense)

    return {
        "model": model,
        "dfc": dfc,
        "summary": summary,
        "silhouette": sil,
        "X_pca": X_pca,
        "K": K,
    }


def seg_label(c: int) -> str:
    return f"Ph√¢n kh√∫c {c + 1}"


def render_header():
    """Ti√™u ƒë·ªÅ + ·∫£nh xe ·ªü g√≥c ph·∫£i."""
    img_path = get_image_path(HEADER_IMAGE_FILE)

    if img_path:
        col1, col2 = st.columns([3, 1])
        with col1:
            st.title("Project 2 ‚Äì ƒê·ªÅ xu·∫•t & ph√¢n kh√∫c xe m√°y ƒë√£ qua s·ª≠ d·ª•ng")
        with col2:
            st.image(img_path, use_column_width=True)
    else:
        st.title("Project 2 ‚Äì ƒê·ªÅ xu·∫•t & ph√¢n kh√∫c xe m√°y ƒë√£ qua s·ª≠ d·ª•ng")


def page_project_intro():
    st.header("Gi·ªõi thi·ªáu d·ª± √°n")
    st.write(
        """
**Project 2: ƒê·ªÅ xu·∫•t xe m√°y d·ª±a tr√™n n·ªôi dung, ph√¢n c·ª•m xe m√°y**

M·ª•c ti√™u:
- X√¢y d·ª±ng h·ªá th·ªëng **ph√¢n kh√∫c xe m√°y ƒë√£ qua s·ª≠ d·ª•ng** b·∫±ng KMeans & PCA;
- X√¢y d·ª±ng h·ªá th·ªëng **ƒë·ªÅ xu·∫•t xe t∆∞∆°ng t·ª±** d·ª±a tr√™n n·ªôi dung (content-based) v·ªõi ma tr·∫≠n **cosine similarity**;
- ·ª®ng d·ª•ng k·∫øt qu·∫£ v√†o h·ªó tr·ª£ quy·∫øt ƒë·ªãnh cho **ng∆∞·ªùi mua** v√† **b√™n b√°n** (gi√°, ph√¢n kh√∫c, l·ª±a ch·ªçn s·∫£n ph·∫©m).

C√°c ch·ª©c nƒÉng ch√≠nh:
1. **Ph√¢n kh√∫c xe m√°y (KMeans)** ‚Äì tr·ª±c quan h√≥a PCA, xem th·ªëng k√™ t·ª´ng ph√¢n kh√∫c & d·ª± ƒëo√°n ph√¢n kh√∫c cho xe m·ªõi.
2. **T√¨m ki·∫øm & g·ª£i √Ω xe t∆∞∆°ng t·ª±** ‚Äì t√¨m xe theo id/t·ª´ kh√≥a, g·ª£i √Ω danh s√°ch xe gi·ªëng nhau v·ªÅ n·ªôi dung.
"""
    )


def page_evaluation(result):
    st.header("ƒê√°nh gi√° & B√°o c√°o (KMeans)")

    st.subheader("1Ô∏è‚É£ Th√¥ng tin m√¥ h√¨nh")
    st.write(f"- S·ªë ph√¢n kh√∫c (K): **{result['K']}**")
    st.write(f"- Gi√° tr·ªã Silhouette: **{result['silhouette']:.4f}**")
    st.markdown(
        """
- Silhouette c√†ng l·ªõn (g·∫ßn 1) ‚Üí c√°c ph√¢n kh√∫c c√†ng t√°ch bi·ªát, ch·∫•t l∆∞·ª£ng ph√¢n c·ª•m c√†ng t·ªët.
"""
    )

    st.subheader("2Ô∏è‚É£ Th·ªëng k√™ theo t·ª´ng ph√¢n kh√∫c (ch·ªâ ti√™u numeric)")
    summary = result["summary"].copy()
    summary.index = [seg_label(i) for i in summary.index]
    st.dataframe(summary, use_container_width=True)


def page_cluster_and_predict(df, numeric_cols, categorical_cols, preprocess, result):
    """
    Trang: Kh√°m ph√° & D·ª± ƒëo√°n ph√¢n kh√∫c
    - Hi·ªÉn th·ªã PCA, b·∫£ng xe theo ph√¢n kh√∫c
    - Form d·ª± ƒëo√°n ph√¢n kh√∫c cho xe m·ªõi
    - SAU KHI D·ª∞ ƒêO√ÅN: Hi·ªán th√¥ng tin chi ti·∫øt v·ªÅ ph√¢n kh√∫c ƒë∆∞·ª£c d·ª± ƒëo√°n
    """
    st.header("Kh√°m ph√° & D·ª± ƒëo√°n ph√¢n kh√∫c (KMeans + PCA)")

    dfc = result["dfc"]
    X_pca = result["X_pca"]
    model = result["model"]

    # ----- PCA plot
    st.subheader("üåà Tr·ª±c quan PCA 2D theo ph√¢n kh√∫c")
    fig, ax = plt.subplots(figsize=(8, 5))
    clusters = sorted(dfc["cluster"].unique())
    colors = plt.cm.viridis(np.linspace(0, 1, len(clusters)))

    for cl, color in zip(clusters, colors):
        mask = dfc["cluster"] == cl
        ax.scatter(
            X_pca[mask, 0],
            X_pca[mask, 1],
            s=10,
            color=color,
            label=seg_label(cl),
        )
    ax.set_xlabel("PC1")
    ax.set_ylabel("PC2")
    ax.legend()
    st.pyplot(fig)

    # ----- B·∫£ng chi ti·∫øt t·ª´ng ph√¢n kh√∫c
    st.subheader("üìÑ Danh s√°ch xe theo ph√¢n kh√∫c")
    choice = st.selectbox(
        "Ch·ªçn ph√¢n kh√∫c mu·ªën xem:",
        clusters,
        format_func=seg_label,
    )
    st.dataframe(
        dfc[dfc["cluster"] == choice].reset_index(drop=True),
        use_container_width=True,
    )

    st.markdown("---")

    # ----- Form d·ª± ƒëo√°n ph√¢n kh√∫c cho xe ng∆∞·ªùi d√πng
    st.subheader("üõµ D·ª± ƒëo√°n ph√¢n kh√∫c cho xe c·ªßa b·∫°n")

    defaults = {c: float(df[c].median()) for c in numeric_cols}
    cats = {c: sorted(df[c].dropna().unique()) for c in categorical_cols}

    with st.form("predict_form"):
        col1, col2, col3 = st.columns(3)

        thuong_hieu = col1.selectbox("Th∆∞∆°ng hi·ªáu", cats.get("Th∆∞∆°ng hi·ªáu", [""]))
        dong_xe = col2.selectbox("D√≤ng xe", cats.get("D√≤ng xe", [""]))
        loai_xe = col3.selectbox("Lo·∫°i xe", cats.get("Lo·∫°i xe", [""]))

        col4, col5, col6 = st.columns(3)
        dung_tich = col4.selectbox("Dung t√≠ch xe", cats.get("Dung t√≠ch xe", [""]))
        xuat_xu = col5.selectbox("Xu·∫•t x·ª©", cats.get("Xu·∫•t x·ª©", [""]))
        gia = col6.number_input(
            "Gi√° (tri·ªáu ƒë·ªìng)", value=defaults.get("Gi√°", 20.0), min_value=0.0
        )

        col7, col8 = st.columns(2)
        nam_dk = col7.number_input(
            "NƒÉm ƒëƒÉng k√Ω",
            min_value=1990,
            max_value=2025,
            value=int(df["NƒÉm ƒëƒÉng k√Ω"].median()),
        )
        so_km = col8.number_input(
            "S·ªë Km ƒë√£ ƒëi",
            value=defaults.get("S·ªë Km ƒë√£ ƒëi", 30000.0),
            min_value=0.0,
            step=1000.0,
        )

        submit = st.form_submit_button("üîç D·ª± ƒëo√°n ph√¢n kh√∫c")

    if submit:
        tuoi_xe = 2025 - nam_dk

        row = {
            "Gi√°": gia,
            "Tu·ªïi xe": tuoi_xe,
            "S·ªë Km ƒë√£ ƒëi": so_km,
            "Th∆∞∆°ng hi·ªáu": thuong_hieu,
            "D√≤ng xe": dong_xe,
            "Lo·∫°i xe": loai_xe,
            "Dung t√≠ch xe": dung_tich,
            "Xu·∫•t x·ª©": xuat_xu,
        }

        X_user = preprocess.transform(pd.DataFrame([row])).toarray()
        pred = int(model.predict(X_user)[0])
        cluster_name = seg_label(pred)

        st.success(f"‚úÖ Xe c·ªßa b·∫°n ƒë∆∞·ª£c x·∫øp v√†o **{cluster_name}**.")

        # ==== TH√îNG TIN CHI TI·∫æT V·ªÄ PH√ÇN KH√öC D·ª∞ ƒêO√ÅN ====
        st.markdown("---")
        st.subheader(f"üìä Th√¥ng tin v·ªÅ {cluster_name}")

        dfc_cluster = dfc[dfc["cluster"] == pred].copy()
        cluster_size = len(dfc_cluster)

        st.write(f"- **S·ªë l∆∞·ª£ng xe** trong {cluster_name}: **{cluster_size}** chi·∫øc")

        # Th·ªëng k√™ c√°c bi·∫øn numeric trong ph√¢n kh√∫c
        if numeric_cols:
            num_stats = (
                dfc_cluster[numeric_cols]
                .agg(["mean", "min", "max"])
                .round(2)
                .T
            )
            num_stats.columns = ["Trung b√¨nh", "Nh·ªè nh·∫•t", "L·ªõn nh·∫•t"]
            st.write("**Th·ªëng k√™ c√°c ch·ªâ ti√™u ƒë·ªãnh l∆∞·ª£ng (trong ph√¢n kh√∫c):**")
            st.dataframe(num_stats, use_container_width=True)

        # Th√¥ng tin ph√¢n b·ªë m·ªôt s·ªë bi·∫øn categorical
        cat_info_lines = []
        for col in categorical_cols:
            if col in dfc_cluster.columns and dfc_cluster[col].notna().any():
                top_val = dfc_cluster[col].value_counts().idxmax()
                pct = (
                    dfc_cluster[col].value_counts(normalize=True).iloc[0] * 100
                )
                cat_info_lines.append(
                    f"- {col}: ph·ªï bi·∫øn nh·∫•t l√† **{top_val}** (~{pct:.1f}%)"
                )

        if cat_info_lines:
            st.write("**ƒê·∫∑c tr∆∞ng ƒë·ªãnh t√≠nh n·ªïi b·∫≠t trong ph√¢n kh√∫c:**")
            st.markdown("\n".join(cat_info_lines))

        # G·ª£i √Ω: hi·ªÉn th·ªã v√†i xe ti√™u bi·ªÉu trong ph√¢n kh√∫c
        st.write("**M·ªôt s·ªë xe ti√™u bi·ªÉu trong ph√¢n kh√∫c:**")
        st.dataframe(
            dfc_cluster.head(10).reset_index(drop=True),
            use_container_width=True,
        )


def page_team():
    st.header("Th√¥ng tin nh√≥m th·ª±c hi·ªán")
    st.write(
        """
**Nh√≥m h·ªçc vi√™n:**
1. Mai B·∫£o Ng·ªçc  
2. B√πi Ng·ªçc To·∫£n  
3. Nguy·ªÖn V≈© Duy  
"""
    )


# ======================================================
# PH·∫¶N 2: T√åM KI·∫æM & ƒê·ªÄ XU·∫§T XE T∆Ø∆†NG T·ª∞ (CONTENT-BASED)
# ======================================================

@st.cache_resource(ttl=3600)
def load_search_data(path):
    """ƒê·ªçc d·ªØ li·ªáu xe cho h·ªá g·ª£i √Ω."""
    try:
        df = pd.read_excel(path, engine="openpyxl")
        df = df.reset_index(drop=True)
        return df
    except Exception as e:
        st.error(f"Kh√¥ng th·ªÉ ƒë·ªçc file d·ªØ li·ªáu: {path}\n{e}")
        return None


@st.cache_resource(ttl=3600)
def load_cosine_raw(path):
    """Th·ª≠ ƒë·ªçc ma tr·∫≠n cosine similarity t·ª´ file .pkl (n·∫øu c√≥)."""
    if not os.path.exists(path):
        return None
    try:
        with open(path, "rb") as f:
            cosine = pickle.load(f)
        return cosine
    except Exception as e:
        st.warning(f"L·ªói khi load ma tr·∫≠n cosine t·ª´ {path}: {e}")
        return None


@st.cache_resource(ttl=3600)
def build_cosine_from_df(df, pkl_path=None):
    """
    X√¢y d·ª±ng ma tr·∫≠n cosine similarity t·ª´ d·ªØ li·ªáu df
    d·ª±a tr√™n n·ªôi dung Ti√™u ƒë·ªÅ + M√¥ t·∫£ chi ti·∫øt.
    N·∫øu pkl_path ƒë∆∞·ª£c cung c·∫•p th√¨ l∆∞u l·∫°i ra file.
    """
    text_series = (
        df.get("Ti√™u ƒë·ªÅ", "").fillna("").astype(str)
        + " "
        + df.get("M√¥ t·∫£ chi ti·∫øt", "").fillna("").astype(str)
    ).str.lower()

    tfidf = TfidfVectorizer(max_features=5000)
    tfidf_matrix = tfidf.fit_transform(text_series)

    cosine_sim = cosine_similarity(tfidf_matrix)

    if pkl_path is not None:
        try:
            with open(pkl_path, "wb") as f:
                pickle.dump(cosine_sim, f)
        except Exception as e:
            st.warning(f"Kh√¥ng th·ªÉ l∆∞u ma tr·∫≠n cosine ra file {pkl_path}: {e}")

    return cosine_sim


def get_or_create_cosine(df_bikes, cosine_path):
    """
    L·∫•y ma tr·∫≠n cosine n·∫øu c√≥, n·∫øu kh√¥ng s·∫Ω t·ª± ƒë·ªông build m·ªõi t·ª´ df_bikes.
    ƒê·ªìng th·ªùi ki·ªÉm tra k√≠ch th∆∞·ªõc c√≥ kh·ªõp s·ªë d√≤ng c·ªßa df_bikes hay kh√¥ng.
    """
    cosine_sim = load_cosine_raw(cosine_path)
    if cosine_sim is None:
        st.info("ƒêang t·∫°o m·ªõi ma tr·∫≠n cosine similarity t·ª´ d·ªØ li·ªáu...")
        cosine_sim = build_cosine_from_df(df_bikes, pkl_path=cosine_path)
    else:
        # n·∫øu s·ªë d√≤ng kh√¥ng kh·ªõp th√¨ build l·∫°i
        try:
            if cosine_sim.shape[0] != len(df_bikes):
                st.info(
                    "K√≠ch th∆∞·ªõc ma tr·∫≠n cosine kh√¥ng kh·ªõp s·ªë d√≤ng d·ªØ li·ªáu. "
                    "ƒêang x√¢y d·ª±ng l·∫°i ma tr·∫≠n cosine..."
                )
                cosine_sim = build_cosine_from_df(df_bikes, pkl_path=cosine_path)
        except Exception:
            st.info(
                "Kh√¥ng ki·ªÉm tra ƒë∆∞·ª£c k√≠ch th∆∞·ªõc ma tr·∫≠n cosine. "
                "ƒêang x√¢y d·ª±ng l·∫°i ma tr·∫≠n cosine..."
            )
            cosine_sim = build_cosine_from_df(df_bikes, pkl_path=cosine_path)

    return cosine_sim


def find_best_title_match(df_titles, query):
    best_idx = None
    best_score = 0.0
    q = str(query).strip().lower()
    if not q:
        return None, 0.0
    for idx, title in enumerate(df_titles):
        t = str(title).lower()
        score = SequenceMatcher(None, q, t).ratio()
        if score > best_score:
            best_score = score
            best_idx = idx
    return best_idx, best_score


def get_recommendations_by_index(df, cosine_sim, idx, top_k=30):
    if cosine_sim is None:
        return pd.DataFrame()
    try:
        sim_scores = list(enumerate(cosine_sim[idx]))
        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
        sim_scores = [s for s in sim_scores if s[0] != idx]
        top_scores = sim_scores[:top_k]
        indices = [i for i, _ in top_scores]
        return df.iloc[indices].reset_index(drop=True)
    except Exception as e:
        st.error(f"L·ªói khi l·∫•y g·ª£i √Ω t·ª´ ma tr·∫≠n cosine: {e}")
        return pd.DataFrame()


def display_rows_with_expander(df_rows):
    if df_rows is None or df_rows.empty:
        st.write("_Kh√¥ng c√≥ k·∫øt qu·∫£ ƒë·ªÉ hi·ªÉn th·ªã._")
        return

    c0, c1, c2, c3, c4, c5 = st.columns([3, 2, 2, 1, 1, 1])
    c0.markdown("**Ti√™u ƒë·ªÅ**")
    c1.markdown("**Th∆∞∆°ng hi·ªáu**")
    c2.markdown("**D√≤ng xe**")
    c3.markdown("**NƒÉm ƒëƒÉng k√Ω**")
    c4.markdown("**Gi√°**")
    c5.markdown("**Chi ti·∫øt**")

    for _, row in df_rows.iterrows():
        t0, t1, t2, t3, t4, t5 = st.columns([3, 2, 2, 1, 1, 1])
        t0.write(row.get("Ti√™u ƒë·ªÅ", ""))
        t1.write(row.get("Th∆∞∆°ng hi·ªáu", ""))
        t2.write(row.get("D√≤ng xe", ""))
        t3.write(row.get("NƒÉm ƒëƒÉng k√Ω", ""))
        t4.write(row.get("Gi√°", ""))
        bike_id = row.get("id", "")
        label = f"Chi ti·∫øt ({bike_id})"
        with t5:
            with st.expander(label):
                desc = row.get("M√¥ t·∫£ chi ti·∫øt", "")
                if desc:
                    st.write(desc)
                else:
                    st.write("_Kh√¥ng c√≥ m√¥ t·∫£ chi ti·∫øt._")


def paginate_dataframe(df, page, per_page):
    if df is None:
        return pd.DataFrame()
    start = (page - 1) * per_page
    end = start + per_page
    return df.iloc[start:end].reset_index(drop=True)


def page_search_and_recommend():
    """Trang T√¨m ki·∫øm & ƒë·ªÅ xu·∫•t xe t∆∞∆°ng t·ª±."""
    img_path = get_image_path(SEARCH_HEADER_IMAGE_FILE)
    if img_path:
        try:
            st.image(img_path, use_column_width=True)
        except Exception:
            pass

    st.header("T√¨m ki·∫øm & ƒê·ªÅ xu·∫•t xe m√°y t∆∞∆°ng t·ª± (Content-based)")

    # load d·ªØ li·ªáu & cosine
    data_path = get_file_path(SEARCH_DATA_FILE)
    cosine_path = get_file_path(COSINE_PKL_FILE)

    df_bikes = load_search_data(data_path)
    if df_bikes is None:
        st.stop()

    cosine_sim = get_or_create_cosine(df_bikes, cosine_path)

    # session init (safe defaults)
    if "random_bikes" not in st.session_state:
        st.session_state.random_bikes = df_bikes.head(10).reset_index(drop=True)
    if "selected_bike_id" not in st.session_state:
        st.session_state.selected_bike_id = None
    if "page" not in st.session_state:
        st.session_state.page = 1
    if "max_results" not in st.session_state:
        st.session_state.max_results = 30
    if "per_page" not in st.session_state:
        st.session_state.per_page = 6
    if "last_query" not in st.session_state:
        st.session_state.last_query = ""
    if "last_query_method" not in st.session_state:
        st.session_state.last_query_method = ""

    # function callbacks
    def refresh_random_list():
        try:
            st.session_state.random_bikes = df_bikes.sample(n=10).reset_index(drop=True)
            st.session_state.selected_bike_id = None
            st.session_state.last_query = ""
            st.session_state.last_query_method = ""
            st.session_state.page = 1
            st.session_state.pop("selected_bike_option", None)
        except Exception as e:
            st.error("L·ªói khi l√†m m·ªõi danh s√°ch: " + str(e))

    def on_select_change():
        val = st.session_state.get("selected_bike_option", None)
        if val:
            try:
                # val l√† tuple (title, id)
                st.session_state.selected_bike_id = val[1]
                st.session_state.last_query = str(st.session_state.selected_bike_id)
                st.session_state.last_query_method = "selectbox"
                st.session_state.page = 1
            except Exception:
                pass

    # --- Search UI: selection A and typed input B ---
    st.markdown("---")
    colA1, colA2 = st.columns([4, 1])
    with colA1:
        bike_options = [
            (row["Ti√™u ƒë·ªÅ"], row["id"])
            for _, row in st.session_state.random_bikes.iterrows()
        ]
        st.selectbox(
            "Danh s√°ch xe ng·∫´u nhi√™n",
            options=bike_options,
            format_func=lambda x: x[0] if isinstance(x, tuple) else str(x),
            key="selected_bike_option",
            on_change=on_select_change,
        )
    with colA2:
        if st.button("L√†m m·ªõi danh s√°ch"):
            refresh_random_list()

    q_input = st.text_input("Nh·∫≠p id ho·∫∑c t·ª´ kh√≥a t√¨m ki·∫øm:", value="")

    # Thi·∫øt l·∫≠p g·ª£i √Ω
    st.markdown("**Thi·∫øt l·∫≠p g·ª£i √Ω**")
    cols_set = st.columns([1, 1, 2])
    with cols_set[0]:
        max_results = st.number_input(
            "S·ªë g·ª£i √Ω t·ªëi ƒëa (t·ªïng)",
            min_value=5,
            max_value=500,
            value=st.session_state.max_results,
            step=5,
            key="input_max_results",
        )
    with cols_set[1]:
        per_page = st.selectbox(
            "S·ªë k·∫øt qu·∫£ / trang",
            options=[3, 4, 6, 10],
            index=[
                3,
                4,
                6,
                10,
            ].index(st.session_state.per_page)
            if st.session_state.per_page in [3, 4, 6, 10]
            else 2,
            key="input_per_page",
        )

    # sync to session_state
    st.session_state.max_results = int(max_results)
    st.session_state.per_page = int(per_page)

    # Button cho t√¨m ki·∫øm g√µ tay
    if st.button("üîç T√¨m ki·∫øm"):
        if str(q_input).strip() == "":
            st.info("H√£y nh·∫≠p id ho·∫∑c t·ª´ kh√≥a v√†o √¥ t√¨m ki·∫øm.")
        else:
            st.session_state.page = 1
            st.session_state.last_query = str(q_input).strip()
            st.session_state.last_query_method = "typed"

    # ------------------ Processing search ------------------
    last_q = st.session_state.get("last_query", "")
    method = st.session_state.get("last_query_method", "")
    if last_q:
        chosen_index = None
        chosen_method = None

        if method == "selectbox":
            # last_q l√† id
            try:
                q_num = int(last_q)
                matches = df_bikes.index[df_bikes["id"] == q_num].tolist()
                if matches:
                    chosen_index = matches[0]
                    chosen_method = f"id ch√≠nh x√°c ({q_num})"
                else:
                    st.warning(f"Kh√¥ng t√¨m th·∫•y id = {q_num} trong d·ªØ li·ªáu.")
            except Exception:
                st.warning("ID ch·ªçn kh√¥ng h·ª£p l·ªá.")
        else:
            # typed: c√≥ th·ªÉ l√† id ho·∫∑c t·ª´ kh√≥a
            if last_q.isdigit():
                q_num = int(last_q)
                matches = df_bikes.index[df_bikes["id"] == q_num].tolist()
                if matches:
                    chosen_index = matches[0]
                    chosen_method = f"id ch√≠nh x√°c ({q_num})"
            if chosen_index is None:
                best_idx, best_score = find_best_title_match(
                    df_bikes["Ti√™u ƒë·ªÅ"].astype(str).tolist(), last_q
                )
                if best_idx is not None and best_score > 0.05:
                    chosen_index = best_idx
                    chosen_method = f"closest title match (score={best_score:.3f})"
                else:
                    st.warning(
                        "Kh√¥ng t√¨m th·∫•y Ti√™u ƒë·ªÅ n√†o gi·ªëng query. H√£y th·ª≠ t·ª´ kh√≥a kh√°c."
                    )
                    chosen_index = None

        # N·∫øu t√¨m ƒë∆∞·ª£c index -> d√πng cosine ƒë·ªÉ l·∫•y g·ª£i √Ω
        if chosen_index is not None:
            st.success(
                f"ƒê√£ ch·ªçn item index = {chosen_index} b·∫±ng ph∆∞∆°ng ph√°p: {chosen_method}"
            )

            recommendations = get_recommendations_by_index(
                df_bikes,
                cosine_sim,
                chosen_index,
                top_k=st.session_state.max_results,
            )
            if recommendations.empty:
                st.write("_Kh√¥ng c√≥ g·ª£i √Ω_")
            else:
                total = len(recommendations)
                total_pages = max(1, ceil(total / st.session_state.per_page))
                st.write(
                    f"T·ªïng g·ª£i √Ω thu ƒë∆∞·ª£c: **{total}** ‚Äî "
                    f"Hi·ªÉn th·ªã **{st.session_state.per_page}** / trang ‚Äî "
                    f"T·ªïng trang: **{total_pages}**"
                )

                # normalize page in session_state
                if st.session_state.page < 1:
                    st.session_state.page = 1
                if st.session_state.page > total_pages:
                    st.session_state.page = total_pages

                # page chooser
                new_page = st.number_input(
                    "Ch·ªçn trang",
                    min_value=1,
                    max_value=total_pages,
                    value=st.session_state.page,
                    step=1,
                    key="ui_page",
                )
                if new_page != st.session_state.page:
                    st.session_state.page = int(new_page)

                # slice and display
                df_page = paginate_dataframe(
                    recommendations,
                    st.session_state.page,
                    st.session_state.per_page,
                )
                display_rows_with_expander(df_page)

                # navigation buttons
                nav_col1, nav_col2, _ = st.columns([1, 1, 4])
                with nav_col1:
                    if st.button("<< Trang tr∆∞·ªõc"):
                        st.session_state.page = max(1, st.session_state.page - 1)
                with nav_col2:
                    if st.button("Trang sau >>"):
                        st.session_state.page = min(
                            total_pages, st.session_state.page + 1
                        )

    st.markdown("---")
    st.caption(
        "Ghi ch√∫: Ma tr·∫≠n cosine ƒë∆∞·ª£c x√¢y d·ª±ng t·ª´ n·ªôi dung Ti√™u ƒë·ªÅ + M√¥ t·∫£ chi ti·∫øt. "
        "ƒê·∫£m b·∫£o th·ª© t·ª± d√≤ng gi·ªØa dataframe v√† ma tr·∫≠n l√† gi·ªëng nhau (df.reset_index(drop=True))."
    )


# ======================================================
# MAIN APP
# ======================================================

def main():
    st.set_page_config(
        page_title="Project 2 ‚Äì ƒê·ªÅ xu·∫•t & ph√¢n kh√∫c xe m√°y",
        layout="wide",
    )

    # Header chung
    render_header()

    # Sidebar: ch·ªçn ch·ª©c nƒÉng
    st.sidebar.markdown("## Menu")
    page = st.sidebar.radio(
        "Ch·ªçn ch·ª©c nƒÉng:",
        [
            "Gi·ªõi thi·ªáu d·ª± √°n",
            "ƒê√°nh gi√° & b√°o c√°o (KMeans)",
            "Kh√°m ph√° & d·ª± ƒëo√°n ph√¢n kh√∫c",
            "T√¨m ki·∫øm & ƒë·ªÅ xu·∫•t xe t∆∞∆°ng t·ª±",
            "Th√¥ng tin nh√≥m",
        ],
    )

    # V·ªõi c√°c trang li√™n quan ph√¢n c·ª•m, c·∫ßn load d·ªØ li·ªáu 1 l·∫ßn
    cluster_data_path = get_file_path(CLUSTER_DATA_FILE)

    # ƒêi·ªÅu h∆∞·ªõng
    if page == "Gi·ªõi thi·ªáu d·ª± √°n":
        page_project_intro()

    elif page in ["ƒê√°nh gi√° & b√°o c√°o (KMeans)", "Kh√°m ph√° & d·ª± ƒëo√°n ph√¢n kh√∫c"]:
        if not os.path.exists(cluster_data_path):
            st.error(
                f"‚ùå Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu ph√¢n c·ª•m: {CLUSTER_DATA_FILE}. "
                "H√£y ki·ªÉm tra l·∫°i t√™n file."
            )
            return

        # Cho ph√©p ch·ªçn K ·ªü sidebar
        K = st.sidebar.slider(
            "S·ªë ph√¢n kh√∫c (K)", min_value=2, max_value=8, value=3
        )

        df, numeric_cols, categorical_cols, preprocess, X_dense = (
            load_and_prepare_cluster_data(cluster_data_path)
        )
        result = run_kmeans(df, numeric_cols, X_dense, K)

        if page == "ƒê√°nh gi√° & b√°o c√°o (KMeans)":
            page_evaluation(result)
        else:
            page_cluster_and_predict(
                df, numeric_cols, categorical_cols, preprocess, result
            )

    elif page == "T√¨m ki·∫øm & ƒë·ªÅ xu·∫•t xe t∆∞∆°ng t·ª±":
        page_search_and_recommend()

    else:
        page_team()


if __name__ == "__main__":
    main()
